{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import hypertopt as hp\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from scipy.optimize import curve_fit\n",
    "import math\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\".\\data\\airline-passengers.csv\")\n",
    "timeseries = df[[\"Passengers\"]].values.astype(\"float32\")\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, hidden_size, seed, lookback):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size * lookback, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        c_0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        x, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x, hn, cn\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, int(hidden_size / 2))\n",
    "        self.linear2 = nn.Linear(int(hidden_size / 2), 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        return x\n",
    "\n",
    "class RESmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RESmodel, self).__init__()\n",
    "        self.linear1 = nn.Linear(5, 30)\n",
    "        self.linear2 = nn.Linear(30, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        return x\n",
    "    \n",
    "class PredModel(nn.Module):\n",
    "    def __init__(self, inputsize):\n",
    "        self.inputsize = inputsize\n",
    "        super(PredModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(inputsize, inputsize*3)\n",
    "        self.linear2 = nn.Linear(inputsize*3, inputsize)\n",
    "        self.linear3 = nn.Linear(inputsize, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear3(x))\n",
    "        return x\n",
    "\n",
    "def train_lstm(model, criterion, optimizer, train_loader):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, _, _ = model(X_batch.float())\n",
    "        loss = criterion(y_pred.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "def train_ann(model, criterion, optimizer, state, y):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(state)\n",
    "    loss = criterion(y_pred.squeeze(), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model\n",
    "\n",
    "def train_res(pred_model, hn_ver_model, cn_ver_model, res_model, pred_ann_model, y_arima_train, criterion, optimizer, train_loader):\n",
    "    res_model.train()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        X_batch, y_batch = data\n",
    "        with torch.no_grad():\n",
    "            pred, hn, cn = pred_model(X_batch)\n",
    "            h_pred = hn_ver_model(hn)\n",
    "            c_pred = cn_ver_model(cn)\n",
    "            pa_pred = pred_ann_model(X_batch)\n",
    "            arima_pred =  y_arima_train[i*train_loader.batch_size:i*train_loader.batch_size+len(y_batch)]\n",
    "            X_joined = torch.stack([pred.view(pred.size(0)), h_pred.view(h_pred.size(1)), c_pred.view(c_pred.size(1)), pa_pred.view(pred.size(0)), arima_pred.view(arima_pred.size(0))], 1)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = res_model(X_joined)\n",
    "        loss = criterion(y_pred.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return res_model\n",
    "\n",
    "def train_ann_pred(model, criterion, optimizer, train_loader):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "def predict_val(pred_model, hn_ver_model, cn_ver_model, res_model, pred_ann_model, y_arima_val, X):\n",
    "    with torch.no_grad():\n",
    "        pred, hn, cn = pred_model(X)\n",
    "        h_pred = hn_ver_model(hn)\n",
    "        c_pred = cn_ver_model(cn)\n",
    "        pa_pred = pred_ann_model(X)\n",
    "        arima_pred =  y_arima_val\n",
    "        X_joined = torch.stack([pred.view(pred.size(0)), h_pred.view(h_pred.size(1)), c_pred.view(c_pred.size(1)), pa_pred.view(pred.size(0)), arima_pred.view(arima_pred.size(0))], 1)\n",
    "        return res_model(X_joined)\n",
    "    \n",
    "def get_arima_train(train, p, d, q, lookback):\n",
    "    arima_model = ARIMA(train, order=(p,d,q))\n",
    "    arima_model = arima_model.fit()\n",
    "    re = arima_model.predict(start=lookback + 1, end=len(train), typ='levels')   \n",
    "    return torch.tensor(np.array(re).astype(np.float32)).squeeze()\n",
    "\n",
    "def get_arima_val(train, val, p, d, q, lookback):\n",
    "    arima_model = ARIMA(train, order=(p,d,q))\n",
    "    arima_model = arima_model.fit()    \n",
    "    start_val = len(train) + lookback + 1\n",
    "    re =  arima_model.predict(start=start_val, end = len(train) + len(val), typ='levels')\n",
    "    return torch.tensor(np.array(re).astype(np.float32)).squeeze()\n",
    "\n",
    "def get_arima_test(train, val, test, p, d, q, lookback):\n",
    "    arima_model = ARIMA(train, order=(p,d,q))\n",
    "    arima_model = arima_model.fit()\n",
    "    start_test = len(train) + len(val) + lookback + 1\n",
    "    re =  arima_model.predict(start=start_test, end= len(train) + len(val) + len(test), typ='levels')\n",
    "    return torch.tensor(np.array(re).astype(np.float32)).squeeze()\n",
    "\n",
    "def train_run(params):\n",
    "    #Extract hyperparameter\n",
    "    hidden_size = int(params[\"hidden_size\"])    \n",
    "    seed = int(params[\"seed\"])\n",
    "    lookback = 14\n",
    "    train_epochs = 10\n",
    "    batch_size = 5\n",
    "    arima_p = 0\n",
    "    arima_d = 0\n",
    "    arima_q = 0 \n",
    "\n",
    "    # Train-test split\n",
    "    train_size = int(len(timeseries) * 0.67)\n",
    "    test_size = len(timeseries) - train_size\n",
    "    train, test = timeseries[:train_size], timeseries[train_size:]\n",
    "\n",
    "    train_size = int(len(train) * 0.67)\n",
    "    val_size = len(train) - train_size\n",
    "    train, val = train[:train_size], train[train_size:]\n",
    "\n",
    "    def create_dataset(dataset, lookback):\n",
    "        X, y = [], []\n",
    "        for i in range(len(dataset)-lookback):\n",
    "            features = dataset[i:i+lookback]\n",
    "            features = np.array([np.float32(array[0]) for array in features])            \n",
    "            target = dataset[i+lookback:i+lookback+1][0][0]\n",
    "            X.append(features)\n",
    "            y.append(target)      \n",
    "        X = torch.tensor(np.array(X).astype(np.float32))\n",
    "        y = torch.tensor(np.array(y).astype(np.float32))\n",
    "        return X, y \n",
    "\n",
    "    \n",
    "    X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "    X_val, y_val = create_dataset(val, lookback=lookback)\n",
    "    X_test, y_test = create_dataset(test, lookback=lookback)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    pred_lstm_model = LSTMModel(hidden_size, seed, lookback)\n",
    "    hn_ver_model = ANNModel(hidden_size)\n",
    "    cn_ver_model = ANNModel(hidden_size)\n",
    "    ver_model = RESmodel()\n",
    "    pred_ann_model = PredModel(lookback)\n",
    "\n",
    "    #ARIMA\n",
    "    y_arima_train= get_arima_train(train, arima_p, arima_d, arima_q, lookback)\n",
    "    l1=  len(y_arima_train)\n",
    "    l2=  len(y_train)\n",
    "    y_arima_val = get_arima_val(train, val, arima_p, arima_d, arima_q, lookback)\n",
    "    l1=  len(y_arima_val)\n",
    "    l2=  len(y_val)\n",
    "    y_arima_test= get_arima_test(train,val, test, arima_p, arima_d, arima_q, lookback)\n",
    "    l1=  len(y_arima_test)\n",
    "    l2=  len(y_test)\n",
    "    #NN\n",
    "    for _ in range(train_epochs):\n",
    "        criterion = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        optimizer_lstm = torch.optim.Adam(pred_lstm_model.parameters(), lr=0.0015, weight_decay=0.0003)\n",
    "        for _ in range(train_epochs):\n",
    "            pred_lstm_model = train_lstm(pred_lstm_model, criterion, optimizer_lstm, train_loader)\n",
    "\n",
    "        optimizer_hn = torch.optim.Adam(hn_ver_model.parameters(), lr=0.0015, weight_decay=0.0003)\n",
    "        for _ in range(train_epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                _, hn, _ = pred_lstm_model(X_batch)\n",
    "                hn_ver_model = train_ann(hn_ver_model, criterion, optimizer_hn, hn, y_batch)\n",
    "\n",
    "        optimizer_cn = torch.optim.Adam(cn_ver_model.parameters(), lr=0.0015, weight_decay=0.0003)\n",
    "        for _ in range(train_epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                _, _, cn = pred_lstm_model(X_batch)\n",
    "                cn_ver_model = train_ann(cn_ver_model, criterion, optimizer_cn, cn, y_batch)\n",
    "\n",
    "        optimizer_pred_ann = torch.optim.Adam(pred_ann_model.parameters(), lr=0.0015, weight_decay=0.0003)\n",
    "        for _ in range(train_epochs):\n",
    "            pred_ann_model = train_ann_pred(pred_ann_model, criterion, optimizer_pred_ann, train_loader)\n",
    "\n",
    "        optimizer_ver = torch.optim.Adam(ver_model.parameters(), lr=0.0015, weight_decay=0.0003)\n",
    "        for _ in range(train_epochs):\n",
    "            ver_model = train_res(pred_lstm_model, hn_ver_model, cn_ver_model, ver_model, pred_ann_model, y_arima_train, criterion, optimizer_ver, train_loader)\n",
    "\n",
    "        pred_lstm_model.eval()\n",
    "        hn_ver_model.eval()\n",
    "        cn_ver_model.eval()\n",
    "        ver_model.eval()\n",
    "        pred_ann_model.eval()        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Validate time series\n",
    "            y_pred = predict_val(pred_lstm_model, hn_ver_model, cn_ver_model, ver_model, pred_ann_model, y_arima_val, X_val)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            max_error = torch.max(torch.abs(y_val - y_pred))            \n",
    "            mse = mean_squared_error(y_val, y_pred)    \n",
    "    return {\n",
    "        \"loss\": mse,\n",
    "        \"status\": STATUS_OK,\n",
    "        # -- store other results like this\n",
    "        \"scores\": {mae, mse, r2, max_error},\n",
    "        \"models\": {pred_lstm_model, hn_ver_model, cn_ver_model, ver_model, pred_ann_model}  \n",
    "    }\n",
    "\n",
    "hyperopt_loops = 5\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_space = {\n",
    "    #'n_estimators': hp.quniform('n_estimators', 10, 200, 10),\n",
    "    #'max_depth': hp.choice('max_depth', [None] + list(range(5, 31, 5))),\n",
    "    #'min_samples_split': hp.quniform('min_samples_split', 2, 11, 1),\n",
    "    #'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 11, 1),\n",
    "    #'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2']),\n",
    "    \"hidden_size\": hp.randint(\"hidden_size\", 1, 3),\n",
    "    \"p\": hp.randint(\"p\", 1, 1000),\n",
    "    \"d\": hp.randint(\"d\", 1, 1000),\n",
    "    \"q\": hp.randint(\"q\", 1, 1000),\n",
    "    \"seed\": hp.randint(\"seed\", 1, 1000),\n",
    "    \"train_epoch\": hp.randint(\"train_epoch\", 1, 1000),\n",
    "    \"batch_size\": hp.randint(\"batch_size\", 1, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "for _ in range(3):\n",
    "    fmin(\n",
    "        train_run,\n",
    "        param_space,\n",
    "        algo=tpe.suggest,\n",
    "        trials=trials,\n",
    "        max_evals=2)\n",
    "    \n",
    "#Restructure\n",
    "scores = list()\n",
    "models = list()\n",
    "for trial in trials.trials: \n",
    "    scores.append(trial[\"result\"][\"scores\"])\n",
    "    models.append(trial[\"result\"][\"models\"])\n",
    "print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 5))  # Adjust figure size if needed\n",
    "x_values = range(len(arr))\n",
    "plt.plot(x_values, arr, marker='o', linestyle='-', color='r', label='Float Values')\n",
    "true_values = np.concatenate((y_train.numpy(), y_test.numpy()))\n",
    "plt.plot(range(len(true_values)), true_values, marker='x', linestyle='-', color='g', label='True Values')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Visualization of List of Float Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
